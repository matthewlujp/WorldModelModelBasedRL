{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.environments import suite_gym, suite_pybullet, wrappers, py_environment, tf_environment, tf_py_environment, utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('step_type', 'reward', 'discount', 'observation')\n",
      "start:\n",
      " TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([0.], dtype=float32)) \n",
      "\n",
      "middle:\n",
      " TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([0.], dtype=float32)) \n",
      "\n",
      "termination:\n",
      " TimeStep(step_type=array(2, dtype=int32), reward=array(1., dtype=float32), discount=array(0., dtype=float32), observation=array([0.], dtype=float32)) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Three types of time_step\n",
    "time_step = ts.restart(np.array([0.], dtype=np.float32))\n",
    "print(time_step._fields)\n",
    "print(\"start:\\n\", time_step, \"\\n\")\n",
    "\n",
    "time_step = ts.transition(np.array([0.], dtype=np.float32), 0)\n",
    "print(\"middle:\\n\", time_step, \"\\n\")\n",
    "\n",
    "time_step = ts.termination(np.array([0.], dtype=np.float32), 1)\n",
    "print(\"termination:\\n\", time_step, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom PyEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJack(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "        \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def time_step_spec(self):\n",
    "        return ts.time_step_spec(self._observation_spec)\n",
    "    \n",
    "    def _reset(self):\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "    \n",
    "    def _step(self, action):\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "        \n",
    "        if action == 1:  # stop draw\n",
    "            self._episode_ended = True\n",
    "        elif action == 0:  # draw\n",
    "            new_card = np.random.randint(1, 11)\n",
    "            self._state += new_card\n",
    "        else:\n",
    "            raise ValueError(\"`actionn` should be 0 or 1 (received {0})\".format(action))\n",
    "            \n",
    "        if self._episode_ended or self._state >= 21:\n",
    "            reward = 21 - self._state if self._state <= 21 else - 21\n",
    "            return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "        else:\n",
    "            return ts.transition(np.array([self._state], dtype=np.int32), reward=0., discount=1.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlackJack()\n",
    "utils.validate_py_environment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset:\n",
      " TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([0], dtype=int32))\n",
      "Draw a card:\n",
      " TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([5], dtype=int32))\n",
      "Draw a card:\n",
      " TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([10], dtype=int32))\n",
      "Draw a card:\n",
      " TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([15], dtype=int32))\n",
      "End round:\n",
      " TimeStep(step_type=array(2, dtype=int32), reward=array(6., dtype=float32), discount=array(0., dtype=float32), observation=array([15], dtype=int32))\n",
      "Final Reward: 6.0\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "get_new_card_action = np.array(0, dtype=np.int32)\n",
    "end_round_action = np.array(1, dtype=np.int32)\n",
    "\n",
    "env = BlackJack()\n",
    "time_step = env.reset()\n",
    "print(\"Reset:\\n\", time_step)\n",
    "cummulative_reward = time_step.reward\n",
    "\n",
    "for _ in range(3):\n",
    "    time_step = env.step(get_new_card_action)\n",
    "    print(\"Draw a card:\\n\", time_step)\n",
    "    cummulative_reward += time_step.reward\n",
    "    \n",
    "time_step = env.step(end_round_action)\n",
    "print(\"End round:\\n\", time_step)\n",
    "cummulative_reward += time_step.reward\n",
    "print(\"Final Reward:\", cummulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conver to TFEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance: <class 'tf_agents.environments.tf_py_environment.TFPyEnvironment'>\n",
      "\n",
      "TimeStep Spec:\n",
      " TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(1,), dtype=tf.int32, name='observation', minimum=array(0, dtype=int32), maximum=array(2147483647, dtype=int32)))\n",
      "\n",
      "Action Spec:\n",
      " BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "print(\"Instance:\", type(tf_env))\n",
    "print(\"\\nTimeStep Spec:\\n\", tf_env.time_step_spec())\n",
    "print(\"\\nAction Spec:\\n\", tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Spec: BoundedArraySpec(shape=(1,), dtype=dtype('float32'), name='action', minimum=-2.0, maximum=2.0)\n",
      "Discretized Action Spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=4)\n",
      "Discretized Action Spec (2): BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=9)\n"
     ]
    }
   ],
   "source": [
    "# Discretization\n",
    "env = suite_gym.load(\"Pendulum-v0\")\n",
    "print(\"Action Spec:\", env.action_spec())\n",
    "\n",
    "discretized_action_env = wrappers.ActionDiscretizeWrapper(env, num_actions=5)\n",
    "print(\"Discretized Action Spec:\", discretized_action_env.action_spec())\n",
    "\n",
    "discretized_action_env_2 = wrappers.ActionDiscretizeWrapper(env, num_actions=10)\n",
    "print(\"Discretized Action Spec (2):\", discretized_action_env_2.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminated at i = 999\n",
      "terminated at i = 499\n"
     ]
    }
   ],
   "source": [
    "# TimeLimit\n",
    "env = suite_pybullet.load(\"HalfCheetahBulletEnv-v0\")\n",
    "env_tm = wrappers.TimeLimit(env, duration=500)\n",
    "\n",
    "# Without limit\n",
    "time_step = env.reset()\n",
    "for i in range(5000):\n",
    "    time_step = env.step(np.array([0.] * 6, dtype=np.float32))\n",
    "    if time_step.is_last():\n",
    "        print(\"terminated at i = {}\".format(i))\n",
    "        break\n",
    "        \n",
    "# With limit\n",
    "time_step = env_tm.reset()\n",
    "for i in range(5000):\n",
    "    time_step = env_tm.step(np.array([0.] * 6, dtype=np.float32))\n",
    "    if time_step.is_last():\n",
    "        print(\"terminated at i = {}\".format(i))\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
