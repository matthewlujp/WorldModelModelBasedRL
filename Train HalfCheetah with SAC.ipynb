{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import base64\n",
    "import imageio\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import IPython\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.drivers import dynamic_step_driver, dynamic_episode_driver\n",
    "from tf_agents.environments import suite_pybullet, tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import actor_distribution_network, normal_projection_network\n",
    "from tf_agents.policies import greedy_policy, random_tf_policy, policy_saver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from utils import send_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep spec:\n",
      " TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(26,), dtype=dtype('float32'), name='observation', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38)) \n",
      "\n",
      "Observation spec:\n",
      " BoundedArraySpec(shape=(26,), dtype=dtype('float32'), name='observation', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38) \n",
      "\n",
      "Action spec:\n",
      " BoundedArraySpec(shape=(6,), dtype=dtype('float32'), name='action', minimum=-1.0, maximum=1.0) \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAADwCAIAAAD+Tyo8AAAU3klEQVR4nO2da8hl11nHn73PO5MPFWO1yJh8EEo1mIIgJZ9KmyYhU0WTGAcFheaGIpoPpVY0YjCFxqBoESPWUsrkhi2ipoVpLakmXphUJK35NpMRWiw16TTJzHkv57Yvay0/7DPve95z2Wdf1uVZa/1/DMP7nnfv53nWs57/ftbZe599EqUUAQD8JHUdAACgOxAwAB4DAQPgMRAwAB6z89w/f4eI7rrl+ur3c6/sHf5a/bz069otO++4dsvOO1qILfLhc44tzuEnw+Fw7aY1r2ADaxvQhonEBg43IIsFsHWD5Nmv/W/9Fg0HU/9K5x37x4DgXQXfPwYE30jA9TtofLH5ODW+aHpQfEYa+fCdjNT58JPhcFizqenXnThF8NyCafI6q2D4BH8k4OZWGjqANVjz15rzABpaayrg+iA679jcpv0dLQy/844YfkOb9ne0PPz5SazOXlf/2nN3jX+N0zWf6XDoms90mHa9fBKrQwTYYNMGq9tgA2ygd4Oj68CtdjO3Datgggx4dRtWwQQZ8Oo2uhytPwu9aevOjjvH19kjgu/pkUMeOnuMJ/ijGzm0GG0YokODPvrVbtBHv9oN+uh3dctlAWtPsc0tnQeA4Tvc0nkA1rZc3PjYZSSbycLGUW3cv4qw8dqN19wL3cSo6e1ZBdN2e1bBtN2eVTCbtmcVTNvt9Ro/toQ2PR6/XGiMCi4abq8xqkhc1N2J1cRBEx9adkFsiA2xre61LGAtRrGXib0su8NervZqteN6Aev1EduONXvFsCOrubC/o+W52H4Sa6tjc/s6cdpnX+8Crt8XAZvet7/TpjdyNLerJSzvXNfvC9fxuLZcouvvhW7lsoNXm7tvtYDh99l9qwUM3+jua94Dmx6SHQtMwoAFWDBqQU8H1hKZFiNbLfAx4lFWtRjB1Jgw0voyUnPTuuywCqaJHVbB6LLDKpgmdlgF08RONyPrn8jRLYKGdjSasmmHYUieZpthSP4WQN11YI3R2DfV0BpPUw2tBW+qoTWephpa62mq0ccJm5tbAtZgDdaMWtvSgdvaJd2BajfY0Bp/g0hgT4NhlPT6GzlauWnoyZBNBGnfJoK0b3OTwY5L6OaODdk0ZNYXm4bM+mLTkFlfbC6abXQduEM0rQKC2baWYbat5VDN1i2h29oiHlMFy7Bs2axDy91v5Gjuo4/xVpaNGkdOelr21zjnhNc9F1qjY9P2Ebwr+6yCZ5WZtva7Gd++hNbiBvZ12bfgIjb7FlyYs9/oOjDDIoMLuHDuooMX7S42Cpjn+OElZi/BDESjl+7XgevtboVtmqw5wnCsOQo4b2u+XtSOY9O+mA+Kefa6+WI+KObZ6+ar6b3QfXxQoLnr7MuyO/hy5cuCO/0fZljEi/qGO7jz192Wy0jBjBMe+XsMfoAmPHZ5rGwNHmXEidPOHp04xTAZOl3y2PTB7oai6ePXidM+fr3LcB+/3g3W0wy3PomlxTe5m2CHruEXfrX7bX0rZYWPOYJruA7PdTsBOxxYT+8OXcO7j6598b6j0dYmeo7EeQDOhx8tVeo657/asXP+OXjfGoCeZ2JtDaIzzsXjPAnOA+gfAwLoH4Oly0iHBJAyxIAY+Mdw7Cw0hxEiDL0xIAyGYWisz3YPtauBiWa0RMIkDETCNgw+keywOpwgElBPzzNDFT3PTrGKpON14J5eF2ElGF1njPkcSohZchBMDR3i6SLgAIa9FsRTD+Kpx0k8LW6l1Hg9k1vKiF9IGhfhCKkJ3EJqGI+ph9ptgmHGiZ96KfREET/BkJ+J2nInlo9DagXONvnOuVf2dE2ilvNSFVrOk1XUR6XnmxmaRKALtuqNITDieiCmWANbFjDb0AmxdQWxdcOL2Lp/tUo9nFNJCK8H2t90cA6P/8x2vw68CeYTTJHNMbGPkHl4xDvCXk/kWILzOCtMfLKPf5CIUAs8g9Qj4HjytUScQZIPGqY4guwr4GgLjhCnbhBnB9I+LoPPTg24gKwdX6bJUJzdQu3SgQ09I8aQJHwpC0JivQqVeETbrgOb67occgGcE7DSmtNKDma/G6kJfiWXEPACfumNQgx4u4BRWIsg5kUQ8yJOYq5bQhtaMFdghkANPk6i0Zg3hW3qqZQ1GH1f6mnknoZNfuqBAkr4mg4czNj0gvNhlvF3Nk1Hvhj8MQGbOxu86lg7/s6312BO12JUSrT4aaRqCe1vpsj8W1PEXw/ir8do/Km1Q4UhvM4+aILvU2w0/u63Um7F9JqZgjgtHMAQAjjGWShUQxNtSsAWJtVC6QdQmmEQxlybGIV+AVtovBTKjAYDZrwh2luxZgH7mIK12FFvAOtnm9iZd78ORtoE7N3IgY/YKQCP+pAeAQfWrwIbjh0Ce8dhrQZ6lkHfh9oFJirCiHqAEXWm84i6d2A770Ur+OcREHTVg87vQDsK2GahhzdbFNyC0z42q4Lzgam1gC2PB4UONmGzNtjWfDsBB9yjAh6aTQJ+D2K5QhoWSVMB22y8FLR6gUbCrpMmo2skYIZxawTq1QuqRSNbW/EWAVtuvBT08rIi+AHaJ2wNU+0AN37Bt5O+ZL+40X5BB6qysVmula/Vcl3fgZ0cY6DeYGDVo8zBYZjLAra/ZqZosk9YP5skkira8kwsy9FQNHmPDdSSUQ5lnB4GEU/GXRHVYF0Rj4aJ6Nwre6lb9078ov0C7bhqgSnUCwwRW2mRiyH3/ThhBxxKKKqpPQSjtozNURt8KuVa0ACjIhIVLWFz1Btv5DCBW/XGWUw8+eDT9yohVakO///Pj31Vu5e7brne1aSfe2XPzqTbE3C06gWLfOjzZ1QhlVBKKCWqH6QspOu49LPp3im9WFpCQ73Rspj82/7ul+fSLaUqpSqVKqUs5H/93guGvDtf+5iuPeMdOPgMbsV5Bjhw51d+TUxLVUhVSFlU6pXympKNuna4kK4wupw224FRu4CIfvbF+2QhZS5lIWRR/S9kIVUuKj2bDsB5HZo7ghgUsPOsEYP2Gzl/U3zs05OPlpNSTkuZlTIT1/5JmQmRCVmIbzz2ooVInFejoVI0JWDn+SIe6uWQB1f81dWHy1Fe7GZilItpOf83K+VMiFkpC0FKfeMTL7kO0x4mCtLIe2AOVctBvdHy5689mF43SJJECSVzUYlWZmW1iq62sS9d52+GycCpaf0dGOo9hEMq7POnr35EZqI8yIvdrNidFbtZuZcVe1k5Lhyqt4LJjGisT50dmEl2gEOe+PqvEhEpIqVUqWQpZVYubuB8zcyhD5O+U9PaBMxHvRymJ0Ief+lXKLkm3eomDamWtnGu3oqQNKxHwFDvKk1ycusz9y78llBy7LeFn5Pk8JVkvmVy9ENCKVGSJCklaUJpkgzSZJCkJwbpyTTZSWUhit282JufTJKZkIVUQufFm/TkgBQppUiqVd0S0WM//0WN7oKhv4Y1fBoJ6l3L1rR88Ol717yaHP5X/Zos/YmuyZmSuZiT6ockqZScpAklSTJIkjRJBgkRyVyUo6IcFWJSiJlQhVirsV4kCan1NpM0+aOfe/7wV1TLWjqnpW8H5jMfrNiu3rO/uPxSpU91+F/1kjrWiuebKaJDbStVyTlJ1LxFV5JOqr2VlDITYlqKSSlmpanbnjaol3PjZbKQrujcinudhWalXj6TsZUPfO4eJdUycukfzX8Qx/+VSh67nVjO70/MhcyEzIWclWJaiklRjvJiPyuGWTHMit2sHOWmb1pcYq16WU1TAAXccQnNauTErCyoQX4+8Nm7jy2Pj3Gsm23obSvbKjX/TylSNFd+IUQmVSkaBa2P+saL4qmhbXK6LKExAfU0yc+XfuKb9/zP+9b8QdGSgKsX1eoG6vBPan7uV1b/X+vPhVRb1K+THx59QopclrksZ9achkfbtXTrDgz1bqVhii7/y41JMn+vmqZ094WfOfrbou7UNf2qwz8ppeYXbObnfhUpqUhuvH5jlHdNn5BlJkVeCfjjD92+dRdU0VYapqhdB+aWd4Y0T5GUpBRVGiZBz//kq3M9J/Su217XGJKJ6nzmP1QxHZbZgcgnopzJMqt67+/++mntvizA6oRWRcNW3OIkFkP1ckt6K244/bqQVJTzf7OcphmNp7Q/oovP3+A6ujqe+tesmA7LbCTyiSimIp+IfCLyUXP1Mpw4T8s7GQ6HumxZhmHGqX2ibr58s6rew0olJFWSnmVqf0z5La9pCUlvon7hsYtESpaZyCeimIh8IsqpLGbnn3qglR2GFUUsi6o+UduX0Eh0czrk6sKpC+/+zk9JRVJSKSgv1Cyn0ZT2Rkq+cNM7P3zJRJydufN3XkrSE6SkFJkoZpWGiejlpx9yHVqw1C/vtyyheao3ML794xcPJmp/rHYP1JV9emuohvtKSiKi4Qs3uY7uiFt/8++L6W4+emt2cHm2fzkfv12ptxvBHIItUJOrOgHzHAxxnfs+XLn5te9fVW8O1dU9NcuP/YmJht//wFmRj4vpXjHbFflYyaPPGAXWfnmW/aavbtkoYJ7DIMbq7Zmx6269VJTr/8REw9WF5uOvJX3UG+pUmqPpF3x7NICQeOeHL226O8utht//wNnVF5N08PLTD9oPJnKWJLAs4KWvD2YFZ/XqStoPnb50cofS9Nr14QWY9OGKJBmcP3u/6ygMwlYFdFwIxwTMOeh4eMcdl647QTsDSpMjJSfJ8qeSHJIk6fmn9Kg3hoOyCQ7zdiRgzuES75nWyF//w8Unv/Dfn/m/Z07s0KDS8GErdqTg4b/fePyF5MsPPuomFOtwFkV1Wmt+IwfnQIm9erVk77P/dLmY7oliIouZFLkShZTlw+/5qJQkFSlFCdEP3tnlsnDP7L354o3VrSbVPdnVYUQquuG0tvs9UX6dSQnpc81zXz/5uReulrMDUUxkMRXlTJYzUWayzJ688ESaUprQIKXU9lfBzvnRO14vBBUl5dUtnwVNMxpNrX5eAmzCwRd8t4W5gPsc/s6+OEkHJ8rsIB9fqT4YIMtZ9bEeJY8+xPvIw2f6RKglgd/+8o1SklSqumMsy2lvrG7/je/1t0zsWwgxLkJHR/XGsE1cTz79xW995tx3i9nubP972ejNYrZbzvbL7KDMRqKYalSvLqaZmuVqOqPxlPbHanigruyqZx8/pcU4/1lme4ix+gXfbeE/rx148vPfTNIBJaksJkopKXJZTMp8LPLp0m0STKRb8d4zb5x/6seEpLygaaYOJiRsP+fDMda+s7sVfJfQDJO1SqsD8188c/7o2q5SSgkliurDtEtb6pWuxkx+4U9Ord4udt+jl7UYZ9vlFuFWlkw7MLc09edTZ6tnmiulJEkpZalErtTyU+ZYdd1V1t7s+ezjp3RpGLSFqYDDQ4p83nWlWPwkQEWS7vz+b93jJLBW3Pfo5ef++JTFJ22xg9tCmuNJLFYJqqHVkk+WmSxnssxW1Js88vAZL9Rb8ZE/vHzyBA0GNFi4S+zMn73d33KQ824adgL2ZRZb8aHsvefu/4O1C+ZHHv4lJyH14cQOnRjMNVzdK1bm47s/+S3XcdmDj4Z5ncTySL1tp/CWvZuFoOqhOUJQUaqsoO+++6Kh8BYxkdV//NQpIUkp+tvsZSWFUlKJ/Cuf/On+lvloYyscypVRB+aQDnOUJRUl5QVluZpmajKjg7HHbyXPfPzys+N/e278kiimopiKfFTO9m/77S+5jis6GAk4bF79kQtZoWa5mj96cqyu7ClWnxBsi8jH1x5JOSpn+/nkaj656jooq3BYLHARsF/tt9vMjaY0mtD+SA0P1FtDtT/WHpdVvvrE+/LxlWJyNR9fyQ7ezMdv04aP/rcihkrQCAsB+zVnnXnjPRffGqrvX1VvDY8efOV1E37xL0/P9t9Yerpdfw37hVsNuxewd+rtM2HDg+Vn1oFVoiqJnrgXcFSsPuc5SWj3ax434ZeffihJBodfN07BPaSSOY4F7N2xtj/V/Q/Vp3zTaz97zfmn7k93TibpIEkH6eCk63Dc4KoJu7wO7KN6tczT+KWb5l8JSpQQ/cAdZr9+wcc8k+v3lt2wn2pnB39Pq0oL77j9UprSYEA7AzqBu9EDwv5Bx00H9lS9PvaECiTcJjaz7fnbLwDixoGAPe0GADTE5sKh6fcD68LTRRF5ftxB2u1jJ+dWO7C/ZQRAW+wcevAeGLAGB/167AnY65nwdyEHHGKhbCwJ2Gv1AtAZ0xq2IWDf1Yv26xbUTw3GBex79gHojzkN4yQW8AC0gU2YFXAAecf6GWjBUCEZFHAA6gVAIyY0bErAYagX7ZcPqKi14D0wAFbRq2EjAg7jYAm4gbpaRb+Ag8ky1s/AEBpLS7OAg1EvAEbRpWGdAoZ6gWlCqjEtGsZJrPVg/Qws0L/MtAk4pEMj4AwqbRE9Ag4sp2i/wBo9i02DgANTLwCW6aNhvAcG/hFez+is4b4CRioB0EK3wusl4PDUC3wBtVfRXcBBZhDtFzikQ/l1FHCQ6gXAOW01jJNYwFdC7SKtNNxFwEgcAEZpXoqtBRyqeoGPoBrbCTjgfKH9AlY0LMgWAg5YvQAwpImGcRIL+E3YfWWrhpsKOPI0AeCK+uJsJOCw1Qt8J+b63C7gmLMDAAdqmjDeA2P9DDxgU5VuETDaL/CCGAp1rYbrBBxtUgDwhY0CjkG9ICRiqNjVfrNewDHkAgAfWdJw1CexsH4GPrJYt2sEjPYLPCWe0j3U8LKAI0wBAP5yTMDxqBeESjw1XHWgqN8DA+A1d91y/ZGA4zl0EdbPIBTmAo5KvSBsoirmlCIbMAAhkUaoXqyfgyeeqsZJLAA8JjoBo/2CkIhOwCASIllFQ8AAeExcAsb6OSpiaMJxCRiAwIhIwGi/IDyS4XDoOgZLxLCg2kTMB6+w5z2iDgxAeMQi4LAPw6CGsFcfsQgYgCCJQsBovyBUohAwiJyAV9EQMAAeE76AsX4GFG4TDl/AAAQMBAyAxwQuYKyfwSFBrqIDFzAAYROygNF+wRLhNeGQBQxA8EDAAHhMsALG+hmsJbBVdLACBiAGwhQw2i+oIaQmHKaAAYgECBgAjwlQwFg/g60Es4oOUMAAxAMEDCIljCYcmoCxfgZREZqAAYiKoASM9gtaEcAqOigBAxAbEDCIGt+b8P8DzNcPX+P/3a4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=320x240 at 0x13EF2EF50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = \"HalfCheetahBulletEnv-v0\"\n",
    "train_env = suite_pybullet.load(env_name)\n",
    "\n",
    "print(\"TimeStep spec:\\n\", train_env.time_step_spec(), \"\\n\")\n",
    "print(\"Observation spec:\\n\", train_env.observation_spec(), \"\\n\")\n",
    "print(\"Action spec:\\n\", train_env.action_spec(), \"\\n\")\n",
    "\n",
    "train_env.reset()\n",
    "Image.fromarray(train_env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('WARNING')\n",
    "debug = True\n",
    "\n",
    "# Hyper Parameters\n",
    "if debug:\n",
    "    initial_collect_steps = 10\n",
    "    collect_steps_per_iteration = 1\n",
    "    replay_buffer_max_len = 1000000\n",
    "    batch_size = 256\n",
    "    critic_learning_rate = 3e-4\n",
    "    actor_learning_rate = 3e-4\n",
    "    alpha_learning_rate = 3e-4\n",
    "    target_update_tau = 0.005\n",
    "    target_update_period = 1\n",
    "    gamma = 0.99\n",
    "    reward_scale_factor = 1.0\n",
    "    gradient_clipping = None\n",
    "    actor_fc_layer_params = (256, 256)\n",
    "    critic_joint_fc_layer_params = (256, 256)\n",
    "    log_interval = 5000\n",
    "    num_eval_episodes = 1\n",
    "    eval_interval = 10\n",
    "else:\n",
    "    initial_collect_steps = 10000\n",
    "    collect_steps_per_iteration = 1\n",
    "    replay_buffer_max_len = 1000000\n",
    "    batch_size = 256\n",
    "    critic_learning_rate = 3e-4\n",
    "    actor_learning_rate = 3e-4\n",
    "    alpha_learning_rate = 3e-4\n",
    "    target_update_tau = 0.005\n",
    "    target_update_period = 1\n",
    "    gamma = 0.99\n",
    "    reward_scale_factor = 1.0\n",
    "    gradient_clipping = None\n",
    "    actor_fc_layer_params = (256, 256)\n",
    "    critic_joint_fc_layer_params = (256, 256)\n",
    "    log_interval = 5000\n",
    "    num_eval_episodes = 3\n",
    "    eval_interval = 5000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_save_dir(save_dir, create_directory=True, replace_if_test=True):\n",
    "    \"\"\"If create_directory is True, check existance and create `save_dir/logs` and `save_dir/ckpt`.\n",
    "    Otherwise, only make director names and return.\n",
    "    \"\"\"\n",
    "    if create_directory and replace_if_test:\n",
    "        # Check if saving directory is valid\n",
    "        if \"test\" in save_dir and os.path.exists(save_dir):\n",
    "            shutil.rmtree(save_dir)\n",
    "        if os.path.exists(save_dir):\n",
    "            raise ValueError(\"Directory {} already exists.\".format(save_dir))\n",
    "\n",
    "    # Create save dir\n",
    "    ckpt_dir = os.path.join(save_dir, 'checkpoints')\n",
    "    model_dir = os.path.join(save_dir, 'models')\n",
    "    log_dir = os.path.join(save_dir, 'logs')\n",
    "    \n",
    "    if create_directory:\n",
    "        os.makedirs(save_dir)\n",
    "        os.makedirs(ckpt_dir)\n",
    "        os.makedirs(model_dir)\n",
    "        os.makedirs(log_dir)\n",
    "        \n",
    "    return ckpt_dir, model_dir, log_dir\n",
    "\n",
    "\n",
    "\n",
    "def setup_agent(tf_train_env):\n",
    "    critic_net = critic_network.CriticNetwork(\n",
    "        input_tensor_spec=(tf_train_env.observation_spec(), tf_train_env.action_spec()),\n",
    "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "        activation_fn=tf.nn.relu,\n",
    "    )\n",
    "    \n",
    "    def normal_projection_net(action_spec, init_means_output_factor=.1):\n",
    "        return normal_projection_network.NormalProjectionNetwork(\n",
    "            sample_spec=action_spec,\n",
    "            init_means_output_factor=init_means_output_factor,\n",
    "            mean_transform=None,\n",
    "            state_dependent_std=True,\n",
    "            std_transform=sac_agent.std_clip_transform,\n",
    "            scale_distribution=True,\n",
    "        )\n",
    "\n",
    "    actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "        input_tensor_spec=tf_train_env.observation_spec(),\n",
    "        output_tensor_spec=tf_train_env.action_spec(),\n",
    "        fc_layer_params=actor_fc_layer_params,\n",
    "        activation_fn=tf.nn.relu,\n",
    "        continuous_projection_net=normal_projection_net,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Setup agent\n",
    "    global_step = tf.Variable(0, dtype=np.int32, name=\"global_step\")\n",
    "    agent = sac_agent.SacAgent(\n",
    "        time_step_spec=tf_train_env.time_step_spec(),\n",
    "        action_spec=tf_train_env.action_spec(),\n",
    "        critic_network=critic_net,\n",
    "        actor_network=actor_net,\n",
    "        actor_optimizer=tf.optimizers.Adam(learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.optimizers.Adam(learning_rate=critic_learning_rate),\n",
    "        alpha_optimizer=tf.optimizers.Adam(learning_rate=alpha_learning_rate),\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        gradient_clipping=gradient_clipping,\n",
    "        train_step_counter=global_step,\n",
    "    )\n",
    "    agent.train = common.function(agent.train)  # optimize execution\n",
    "    \n",
    "    print(\"Critic Network:\\n\")\n",
    "    critic_net.summary()\n",
    "    print(\"\\n\\nActor Network:\\n\")\n",
    "    actor_net.summary()\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n",
    "\n",
    "def train(save_dir, train_iterations=100000, checkpoint_dir=None, \n",
    "          notification_addr=None, notification_addr_passwd=None):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ---\n",
    "    checkpoint_dir: If specified, load data from the latest checkpoint and resume training.\n",
    "    \"\"\"\n",
    "    ckpt_dir, model_dir, log_dir = create_save_dir(save_dir, create_directory=(checkpoint_dir is None))\n",
    "    if checkpoint_dir is not None:\n",
    "        assert os.path.normpath(ckpt_dir) == os.path.normpath(checkpoint_dir),\\\n",
    "            \"ckpt_dir {} != checkpoint_dir {}\".format(ckpt_dir, checkpoint_dir)\n",
    "    \n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    \n",
    "    \n",
    "    training_start_time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "    \n",
    "    \n",
    "    env_name = \"HalfCheetahBulletEnv-v0\"\n",
    "    train_env, eval_env = suite_pybullet.load(env_name), suite_pybullet.load(env_name)\n",
    "    tf_train_env, tf_eval_env = tf_py_environment.TFPyEnvironment(train_env), tf_py_environment.TFPyEnvironment(eval_env)\n",
    "\n",
    "\n",
    "    # Setup networks\n",
    "    agent = setup_agent(tf_train_env)\n",
    "\n",
    "\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=agent.collect_data_spec,\n",
    "        batch_size=tf_train_env.batch_size,\n",
    "        max_length=replay_buffer_max_len)\n",
    "\n",
    "    \n",
    "    \n",
    "    metrics = {\n",
    "        'epoch': tf.Variable([], shape=(None,), name='epoch'),\n",
    "        'loss': tf.Variable([], shape=(None,), name='loss'),\n",
    "        'eval_epoch': tf.Variable([], shape=(None,), name='eval_epoch'),\n",
    "        'eval_return': tf.Variable([], shape=(None,), name='eval_return'),\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Setup a Checkpointer\n",
    "    checkpointer = common.Checkpointer(\n",
    "        ckpt_dir=ckpt_dir, max_to_keep=3,\n",
    "        agent=agent, policy=agent.policy, replay_buffer=replay_buffer,\n",
    "        global_step=agent.train_step_counter, **metrics)\n",
    "    checkpointer.initialize_or_restore()\n",
    "    \n",
    "    # Setup policy saver\n",
    "    p_saver = policy_saver.PolicySaver(agent.policy)\n",
    "    \n",
    "\n",
    "    # Setupt training\n",
    "    def evaluate_policy(env, policy, num_episode=5):\n",
    "        avg_return = tf_metrics.AverageReturnMetric()\n",
    "        eval_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "            env, policy, observers=[avg_return], num_episodes=num_episode)\n",
    "        eval_driver.run()\n",
    "        return avg_return.result().numpy()\n",
    "\n",
    "\n",
    "    initial_step = agent.train_step_counter.value().numpy()\n",
    "    if initial_step == 0:\n",
    "        print(\"\\n\\nEvaluate agent policy before training\")\n",
    "        avg_return = evaluate_policy(tf_eval_env, agent.policy, num_eval_episodes)\n",
    "        print(\"Avg.return (before training): {:.1f}\".format(avg_return))\n",
    "\n",
    "        # Collect initial samples\n",
    "        initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "            tf_train_env, agent.collect_policy, observers=[replay_buffer.add_batch], num_steps=initial_collect_steps)\n",
    "        initial_collect_driver.run()\n",
    "    else:\n",
    "        print('Global step {}. Skip initial evaluation and data collection.'.format(initial_step))\n",
    "    \n",
    "    \n",
    "    steps_in_train_env = tf_metrics.EnvironmentSteps()\n",
    "    collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        tf_train_env, agent.collect_policy,\n",
    "        observers=[steps_in_train_env, replay_buffer.add_batch],\n",
    "        num_steps=collect_steps_per_iteration)\n",
    "\n",
    "    \n",
    "    dataset = replay_buffer.as_dataset(sample_batch_size=batch_size, num_steps=2, num_parallel_calls=3).prefetch(3)\n",
    "    dataset_iterator = iter(dataset)\n",
    "\n",
    "        \n",
    "    # Start trining loop\n",
    "    for _ in tqdm(range(initial_step, train_iterations), initial=initial_step, total=train_iterations):\n",
    "        global_step = agent.train_step_counter.value().numpy()\n",
    "        \n",
    "        collect_driver.run()\n",
    "        experiences, _ = next(dataset_iterator)\n",
    "        loss = agent.train(experiences)\n",
    "        \n",
    "        metrics['epoch'].assign(np.append(metrics['epoch'].value(), global_step))\n",
    "        metrics['loss'].assign(np.append(metrics['loss'].value(), loss.loss))\n",
    "\n",
    "        \n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"loss\", loss.loss, step=global_step)\n",
    "            \n",
    "        checkpointer.save(agent.train_step_counter)\n",
    "        \n",
    "        if global_step % eval_interval == 0:\n",
    "            avg_return = evaluate_policy(tf_eval_env, agent.policy, num_eval_episodes)\n",
    "            metrics['eval_epoch'].assign(np.append(metrics['eval_epoch'].value(), global_step))\n",
    "            metrics['eval_return'].assign(np.append(metrics['eval_return'].value(), avg_return))\n",
    "\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar(\"return\", avg_return, step=global_step)\n",
    "                tf.summary.scalar(\"return - step\", avg_return, step=steps_in_train_env.result().numpy())\n",
    "    \n",
    "            # save model\n",
    "            p_saver.save(os.path.join(model_dir, \"latest_model\"))\n",
    "            \n",
    "    \n",
    "    # Send notification\n",
    "    if notification_addr and notification_addr_passwd:\n",
    "        send_email(\"Training started at {} done.\".format(training_start_time),\n",
    "                   notification_addr_passwd, notification_addr)\n",
    "    return agent.policy\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Evaluate and visualize\n",
    "def embed_mp4(filename):\n",
    "    video = open(filename, 'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "    return IPython.display.HTML(tag)\n",
    "\n",
    "\n",
    "def evaluate_policy_and_save_video(py_env, policy, video_filename, num_episodes=5, fps=30):\n",
    "    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "    \n",
    "    with imageio.get_writer(video_filename, fps=fps) as video:\n",
    "        # Run policy\n",
    "        returns = []\n",
    "        \n",
    "        for i in range(num_episodes):\n",
    "            time_step = tf_env.reset()\n",
    "            video.append_data(py_env.render())\n",
    "            episode_return = 0.\n",
    "            \n",
    "            while not time_step.is_last():\n",
    "                action_step = policy.action(time_step)\n",
    "                time_step = tf_env.step(action_step.action)\n",
    "                episode_return += time_step.reward\n",
    "                video.append_data(py_env.render())\n",
    "                \n",
    "            returns.append(episode_return)\n",
    "            print(\"Eval Episode {} --- Return: {}\".format(i, episode_return))\n",
    "            \n",
    "        print(\"Avg. return: {:.1f}(std {:.1f})\".format(np.mean(returns), np.std(returns)))\n",
    "                \n",
    "    return embed_mp4(video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critic Network:\n",
      "\n",
      "Model: \"CriticNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_39 (Flatten)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "joint_mlp/dense (Dense)      multiple                  8448      \n",
      "_________________________________________________________________\n",
      "joint_mlp/dense (Dense)      multiple                  65792     \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  257       \n",
      "=================================================================\n",
      "Total params: 74,497\n",
      "Trainable params: 74,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Actor Network:\n",
      "\n",
      "Model: \"ActorDistributionNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  72704     \n",
      "_________________________________________________________________\n",
      "NormalProjectionNetwork (Nor multiple                  3084      \n",
      "=================================================================\n",
      "Total params: 75,788\n",
      "Trainable params: 75,788\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Evaluate agent policy before training\n",
      "Avg.return (before training): -1361.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Episode 0 --- Return: [-1388.5879]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e7116a140bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuite_pybullet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HalfCheetahBulletEnv-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mevaluate_policy_and_save_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"evaluation.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-03acd17640d2>\u001b[0m in \u001b[0;36mevaluate_policy_and_save_video\u001b[0;34m(py_env, policy, video_filename, num_episodes, fps)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    480\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         distribution_step.action)\n\u001b[0m\u001b[1;32m    483\u001b[0m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit_log_probability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m     actions = tf.nest.map_structure(\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m         distribution_step.action)\n\u001b[1;32m    483\u001b[0m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/distributions/reparameterized_sampling.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(distribution, reparam, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/distributions/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape, seed, name)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m\"\"\"Generates samples from the wrapped TransformedDistribution.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_squashed_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"log_prob\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mprepended\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m     \"\"\"\n\u001b[0;32m--> 937\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_sample_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py\u001b[0m in \u001b[0;36m_call_sample_n\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m       \u001b[0;31m# event that we need to reinterpret the samples as part of the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m       \u001b[0;31m# event_shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdistribution_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m       \u001b[0;31m# Next, we reshape `x` into its final form. We do this prior to the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py\u001b[0m in \u001b[0;36m_sample_n\u001b[0;34m(self, n, seed, **distribution_kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0moverride_event_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mdistribution_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpick_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneeds_rotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     ], axis=0)\n\u001b[0m\u001b[1;32m    444\u001b[0m     x = self.distribution.sample(sample_shape=sample_shape, seed=seed,\n\u001b[1;32m    445\u001b[0m                                  **distribution_kwargs)\n",
      "\u001b[0;32m<decorator-gen-145>\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow_probability/python/internal/prefer_static.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     64\u001b[0m                      for arg, arg_ in zip(flat_args, flat_args_))\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall_static\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mstatic_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m   \"\"\"\n\u001b[0;32m--> 552\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    504\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     final_index, packed = _packed_nest_with_indices(structure, flat_sequence,\n\u001b[0;32m--> 506\u001b[0;31m                                                     0, is_seq, sequence_fn)\n\u001b[0m\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_packed_nest_with_indices\u001b[0;34m(structure, flat, index, is_seq, sequence_fn)\u001b[0m\n\u001b[1;32m    469\u001b[0m       new_index, child = _packed_nest_with_indices(s, flat, index, is_seq,\n\u001b[1;32m    470\u001b[0m                                                    sequence_fn)\n\u001b[0;32m--> 471\u001b[0;31m       \u001b[0mpacked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m       \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_sequence_like\u001b[0;34m(instance, args)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m   \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0m_is_mutable_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;31m# Pack dictionaries in a deterministic order by sorting the keys.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# Notice this means that we ignore the original order of `OrderedDict`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_dir_name = \"test\"\n",
    "save_dir_path = os.path.join(\"results\", save_dir_name)\n",
    "\n",
    "addr = None\n",
    "passwd = None\n",
    "\n",
    "!rm -r $save_dir_path\n",
    "trained_policy = train(save_dir_path, train_iterations=1, notification_addr=addr, notification_addr_passwd=passwd)\n",
    "# trained_policy = train(\"results/sac_halfcheetah_1\", checkpoint_dir=\"results/sac_halfcheetah_1/checkpoints\", \n",
    "#                        train_iterations=100000, notification_addr=addr, notification_addr_passwd=passwd)\\\n",
    "\n",
    "eval_env = suite_pybullet.load(\"HalfCheetahBulletEnv-v0\")\n",
    "evaluate_policy_and_save_video(eval_env, trained_policy, os.path.join(save_dir_path, \"evaluation.mp4\"), num_episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
