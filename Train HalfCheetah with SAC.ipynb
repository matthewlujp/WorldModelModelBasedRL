{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import base64\n",
    "import imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import IPython\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.drivers import dynamic_step_driver, dynamic_episode_driver\n",
    "from tf_agents.environments import suite_pybullet, tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import actor_distribution_network, normal_projection_network\n",
    "from tf_agents.policies import greedy_policy, random_tf_policy, policy_saver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep spec:\n",
      " TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(26,), dtype=dtype('float32'), name='observation', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38)) \n",
      "\n",
      "Observation spec:\n",
      " BoundedArraySpec(shape=(26,), dtype=dtype('float32'), name='observation', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38) \n",
      "\n",
      "Action spec:\n",
      " BoundedArraySpec(shape=(6,), dtype=dtype('float32'), name='action', minimum=-1.0, maximum=1.0) \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=320x240 at 0x13EF2EF50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = \"HalfCheetahBulletEnv-v0\"\n",
    "train_env = suite_pybullet.load(env_name)\n",
    "\n",
    "print(\"TimeStep spec:\\n\", train_env.time_step_spec(), \"\\n\")\n",
    "print(\"Observation spec:\\n\", train_env.observation_spec(), \"\\n\")\n",
    "print(\"Action spec:\\n\", train_env.action_spec(), \"\\n\")\n",
    "\n",
    "train_env.reset()\n",
    "Image.fromarray(train_env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('WARNING')\n",
    "debug = False\n",
    "\n",
    "# Hyper Parameters\n",
    "if debug:\n",
    "    initial_collect_steps = 10\n",
    "    collect_steps_per_iteration = 1\n",
    "    replay_buffer_max_len = 1000000\n",
    "    batch_size = 256\n",
    "    critic_learning_rate = 3e-4\n",
    "    actor_learning_rate = 3e-4\n",
    "    alpha_learning_rate = 3e-4\n",
    "    target_update_tau = 0.005\n",
    "    target_update_period = 1\n",
    "    gamma = 0.99\n",
    "    reward_scale_factor = 1.0\n",
    "    gradient_clipping = None\n",
    "    actor_fc_layer_params = (256, 256)\n",
    "    critic_joint_fc_layer_params = (256, 256)\n",
    "    log_interval = 5000\n",
    "    num_eval_episodes = 1\n",
    "    eval_interval = 10\n",
    "else:\n",
    "    initial_collect_steps = 10000\n",
    "    collect_steps_per_iteration = 1\n",
    "    replay_buffer_max_len = 1000000\n",
    "    batch_size = 256\n",
    "    critic_learning_rate = 3e-4\n",
    "    actor_learning_rate = 3e-4\n",
    "    alpha_learning_rate = 3e-4\n",
    "    target_update_tau = 0.005\n",
    "    target_update_period = 1\n",
    "    gamma = 0.99\n",
    "    reward_scale_factor = 1.0\n",
    "    gradient_clipping = None\n",
    "    actor_fc_layer_params = (256, 256)\n",
    "    critic_joint_fc_layer_params = (256, 256)\n",
    "    log_interval = 5000\n",
    "    num_eval_episodes = 3\n",
    "    eval_interval = 5000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_save_dir(save_dir, create_directory=True, replace_if_test=True):\n",
    "    \"\"\"If create_directory is True, check existance and create `save_dir/logs` and `save_dir/ckpt`.\n",
    "    Otherwise, only make director names and return.\n",
    "    \"\"\"\n",
    "    if create_directory and replace_if_test:\n",
    "        # Check if saving directory is valid\n",
    "        if \"test\" in save_dir and os.path.exists(save_dir):\n",
    "            shutil.rmtree(save_dir)\n",
    "        if os.path.exists(save_dir):\n",
    "            raise ValueError(\"Directory {} already exists.\".format(save_dir))\n",
    "\n",
    "    # Create save dir\n",
    "    ckpt_dir = os.path.join(save_dir, 'checkpoints')\n",
    "    model_dir = os.path.join(save_dir, 'models')\n",
    "    log_dir = os.path.join(save_dir, 'logs')\n",
    "    \n",
    "    if create_directory:\n",
    "        os.makedirs(save_dir)\n",
    "        os.makedirs(ckpt_dir)\n",
    "        os.makedirs(model_dir)\n",
    "        os.makedirs(log_dir)\n",
    "        \n",
    "    return ckpt_dir, model_dir, log_dir\n",
    "\n",
    "\n",
    "\n",
    "def setup_agent(tf_train_env):\n",
    "    critic_net = critic_network.CriticNetwork(\n",
    "        input_tensor_spec=(tf_train_env.observation_spec(), tf_train_env.action_spec()),\n",
    "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "        activation_fn=tf.nn.relu,\n",
    "    )\n",
    "    \n",
    "    def normal_projection_net(action_spec, init_means_output_factor=.1):\n",
    "        return normal_projection_network.NormalProjectionNetwork(\n",
    "            sample_spec=action_spec,\n",
    "            init_means_output_factor=init_means_output_factor,\n",
    "            mean_transform=None,\n",
    "            state_dependent_std=True,\n",
    "            std_transform=sac_agent.std_clip_transform,\n",
    "            scale_distribution=True,\n",
    "        )\n",
    "\n",
    "    actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "        input_tensor_spec=tf_train_env.observation_spec(),\n",
    "        output_tensor_spec=tf_train_env.action_spec(),\n",
    "        fc_layer_params=actor_fc_layer_params,\n",
    "        activation_fn=tf.nn.relu,\n",
    "        continuous_projection_net=normal_projection_net,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Setup agent\n",
    "    global_step = tf.Variable(0, dtype=np.int32, name=\"global_step\")\n",
    "    agent = sac_agent.SacAgent(\n",
    "        time_step_spec=tf_train_env.time_step_spec(),\n",
    "        action_spec=tf_train_env.action_spec(),\n",
    "        critic_network=critic_net,\n",
    "        actor_network=actor_net,\n",
    "        actor_optimizer=tf.optimizers.Adam(learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.optimizers.Adam(learning_rate=critic_learning_rate),\n",
    "        alpha_optimizer=tf.optimizers.Adam(learning_rate=alpha_learning_rate),\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        gradient_clipping=gradient_clipping,\n",
    "        train_step_counter=global_step,\n",
    "    )\n",
    "    agent.train = common.function(agent.train)  # optimize execution\n",
    "    \n",
    "    print(\"Critic Network:\\n\")\n",
    "    critic_net.summary()\n",
    "    print(\"\\n\\nActor Network:\\n\")\n",
    "    actor_net.summary()\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n",
    "\n",
    "def train(save_dir, train_iterations=100000, checkpoint_dir=None):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ---\n",
    "    checkpoint_dir: If specified, load data from the latest checkpoint and resume training.\n",
    "    \"\"\"\n",
    "    ckpt_dir, model_dir, log_dir = create_save_dir(save_dir, create_directory=(checkpoint_dir is None))\n",
    "    if checkpoint_dir is not None:\n",
    "        assert os.path.normpath(ckpt_dir) == os.path.normpath(checkpoint_dir),\\\n",
    "            \"ckpt_dir {} != checkpoint_dir {}\".format(ckpt_dir, checkpoint_dir)\n",
    "    \n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    \n",
    "    \n",
    "    env_name = \"HalfCheetahBulletEnv-v0\"\n",
    "    train_env, eval_env = suite_pybullet.load(env_name), suite_pybullet.load(env_name)\n",
    "    tf_train_env, tf_eval_env = tf_py_environment.TFPyEnvironment(train_env), tf_py_environment.TFPyEnvironment(eval_env)\n",
    "\n",
    "\n",
    "    # Setup networks\n",
    "    agent = setup_agent(tf_train_env)\n",
    "\n",
    "\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=agent.collect_data_spec,\n",
    "        batch_size=tf_train_env.batch_size,\n",
    "        max_length=replay_buffer_max_len)\n",
    "\n",
    "    \n",
    "    \n",
    "    metrics = {\n",
    "        'epoch': tf.Variable([], shape=(None,), name='epoch'),\n",
    "        'loss': tf.Variable([], shape=(None,), name='loss'),\n",
    "        'eval_epoch': tf.Variable([], shape=(None,), name='eval_epoch'),\n",
    "        'eval_return': tf.Variable([], shape=(None,), name='eval_return'),\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Setup a Checkpointer\n",
    "    checkpointer = common.Checkpointer(\n",
    "        ckpt_dir=ckpt_dir, max_to_keep=3,\n",
    "        agent=agent, policy=agent.policy, replay_buffer=replay_buffer,\n",
    "        global_step=agent.train_step_counter, **metrics)\n",
    "    checkpointer.initialize_or_restore()\n",
    "    \n",
    "    # Setup policy saver\n",
    "    p_saver = policy_saver.PolicySaver(agent.policy)\n",
    "    \n",
    "\n",
    "    # Setupt training\n",
    "    def evaluate_policy(env, policy, num_episode=5):\n",
    "        avg_return = tf_metrics.AverageReturnMetric()\n",
    "        eval_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "            env, policy, observers=[avg_return], num_episodes=num_episode)\n",
    "        eval_driver.run()\n",
    "        return avg_return.result().numpy()\n",
    "\n",
    "\n",
    "    initial_step = agent.train_step_counter.value().numpy()\n",
    "    if initial_step == 0:\n",
    "        print(\"\\n\\nEvaluate agent policy before training\")\n",
    "        avg_return = evaluate_policy(tf_eval_env, agent.policy, num_eval_episodes)\n",
    "        print(\"Avg.return (before training): {:.1f}\".format(avg_return))\n",
    "\n",
    "        # Collect initial samples\n",
    "        initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "            tf_train_env, agent.collect_policy, observers=[replay_buffer.add_batch], num_steps=initial_collect_steps)\n",
    "        initial_collect_driver.run()\n",
    "    else:\n",
    "        print('Global step {}. Skip initial evaluation and data collection.'.format(initial_step))\n",
    "    \n",
    "    \n",
    "    steps_in_train_env = tf_metrics.EnvironmentSteps()\n",
    "    collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        tf_train_env, agent.collect_policy,\n",
    "        observers=[steps_in_train_env, replay_buffer.add_batch],\n",
    "        num_steps=collect_steps_per_iteration)\n",
    "\n",
    "    \n",
    "    dataset = replay_buffer.as_dataset(sample_batch_size=batch_size, num_steps=2, num_parallel_calls=3).prefetch(3)\n",
    "    dataset_iterator = iter(dataset)\n",
    "\n",
    "        \n",
    "    # Start trining loop\n",
    "    for _ in tqdm(range(initial_step, train_iterations), initial=initial_step, total=train_iterations):\n",
    "        global_step = agent.train_step_counter.value().numpy()\n",
    "        \n",
    "        collect_driver.run()\n",
    "        experiences, _ = next(dataset_iterator)\n",
    "        loss = agent.train(experiences)\n",
    "        \n",
    "        metrics['epoch'].assign(np.append(metrics['epoch'].value(), global_step))\n",
    "        metrics['loss'].assign(np.append(metrics['loss'].value(), loss.loss))\n",
    "\n",
    "        \n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"loss\", loss.loss, step=global_step)\n",
    "            \n",
    "        checkpointer.save(agent.train_step_counter)\n",
    "        \n",
    "        if global_step % eval_interval == 0:\n",
    "            avg_return = evaluate_policy(tf_eval_env, agent.policy, num_eval_episodes)\n",
    "            metrics['eval_epoch'].assign(np.append(metrics['eval_epoch'].value(), global_step))\n",
    "            metrics['eval_return'].assign(np.append(metrics['eval_return'].value(), avg_return))\n",
    "\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar(\"return\", avg_return, step=global_step)\n",
    "                tf.summary.scalar(\"return - step\", avg_return, step=steps_in_train_env.result().numpy())\n",
    "    \n",
    "            # save model\n",
    "            p_saver.save(os.path.join(model_dir, \"latest_model\"))\n",
    "    \n",
    "    return agent.policy\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Evaluate and visualize\n",
    "def embed_mp4(filename):\n",
    "    video = open(filename, 'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "    return IPython.display.HTML(tag)\n",
    "\n",
    "\n",
    "def evaluate_policy_and_save_video(py_env, policy, video_filename, num_episodes=5, fps=30):\n",
    "    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "    \n",
    "    with imageio.get_writer(video_filename, fps=fps) as video:\n",
    "        # Run policy\n",
    "        returns = []\n",
    "        \n",
    "        for i in range(num_episodes):\n",
    "            time_step = tf_env.reset()\n",
    "            video.append_data(py_env.render())\n",
    "            episode_return = 0.\n",
    "            \n",
    "            while not time_step.is_last():\n",
    "                action_step = policy.action(time_step)\n",
    "                time_step = tf_env.step(action_step.action)\n",
    "                episode_return += time_step.reward\n",
    "                video.append_data(py_env.render())\n",
    "                \n",
    "            returns.append(episode_return)\n",
    "            print(\"Eval Episode {} --- Return: {}\".format(i, episode_return))\n",
    "            \n",
    "        print(\"Avg. return: {:.1f}(std {:.1f})\".format(np.mean(returns), np.std(returns)))\n",
    "                \n",
    "    return embed_mp4(video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: results/sac_halfcheetah_1: No such file or directory\n",
      "Critic Network:\n",
      "\n",
      "Model: \"CriticNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_13 (Flatten)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "joint_mlp/dense (Dense)      multiple                  8448      \n",
      "_________________________________________________________________\n",
      "joint_mlp/dense (Dense)      multiple                  65792     \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  257       \n",
      "=================================================================\n",
      "Total params: 74,497\n",
      "Trainable params: 74,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Actor Network:\n",
      "\n",
      "Model: \"ActorDistributionNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  72704     \n",
      "_________________________________________________________________\n",
      "NormalProjectionNetwork (Nor multiple                  3084      \n",
      "=================================================================\n",
      "Total params: 75,788\n",
      "Trainable params: 75,788\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Evaluate agent policy before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg.return (before training): -1348.0\n",
      "WARNING:tensorflow:From /Users/matthewishige/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/matthewishige/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "  0%|          | 20/100000 [00:38<53:57:51,  1.94s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-dc20add872a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm -r $save_dir_path'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrained_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# # trained_policy = train(\"results/sac_halfcheetah_1\", checkpoint_dir=\"results/sac_halfcheetah_1/checkpoints\", train_iterations=168)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1880507081d4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(save_dir, train_iterations, checkpoint_dir)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mavg_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_eval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_eval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval_return'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval_return'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1880507081d4>\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(env, policy, num_episode)\u001b[0m\n\u001b[1;32m    178\u001b[0m         eval_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n\u001b[1;32m    179\u001b[0m             env, policy, observers=[avg_return], num_episodes=num_episode)\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0meval_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mavg_return\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/drivers/dynamic_episode_driver.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, time_step, policy_state, num_episodes, maximum_iterations)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         maximum_iterations=maximum_iterations)\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   def _run(self,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/drivers/dynamic_episode_driver.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, time_step, policy_state, num_episodes, maximum_iterations)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             name='driver_loop'))\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 574\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2489\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2490\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2491\u001b[0;31m       return_same_structure=True)\n\u001b[0m\u001b[1;32m   2492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2725\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2726\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2727\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2728\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/drivers/dynamic_episode_driver.py\u001b[0m in \u001b[0;36mloop_body\u001b[0;34m(counter, time_step, policy_state)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mloop_vars\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnext\u001b[0m \u001b[0miteration\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhile_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m       \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0;31m# TODO(b/134487572): TF2 while_loop seems to either ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \"\"\"\n\u001b[1;32m    478\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ppo_policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/policies/actor_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# Actor network outputs nested structure of distributions or actions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     actions_or_distributions, policy_state = self._apply_actor_network(\n\u001b[0;32m--> 149\u001b[0;31m         network_observation, time_step.step_type, policy_state, mask=mask)\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_to_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_or_distribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/policies/actor_policy.py\u001b[0m in \u001b[0;36m_apply_actor_network\u001b[0;34m(self, observation, step_type, policy_state, mask)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m       return self._actor_network(\n\u001b[0;32m--> 124\u001b[0;31m           observation, step_type, policy_state, training=self._training)\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m       return self._actor_network(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnetwork_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/networks/actor_distribution_network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, observations, step_type, network_state, training, mask)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     output_actions = tf.nest.map_structure(\n\u001b[0;32m--> 182\u001b[0;31m         call_projection_net, self._projection_networks)\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/networks/actor_distribution_network.py\u001b[0m in \u001b[0;36mcall_projection_net\u001b[0;34m(proj_net)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_projection_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       distribution, _ = proj_net(\n\u001b[0;32m--> 178\u001b[0;31m           state, outer_rank, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    179\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnetwork_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tf_agents/networks/normal_projection_network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, outer_rank, training, mask)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_dependent_std\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m       \u001b[0mstds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stddev_projection_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0mstds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m--> 967\u001b[0;31m               self._compute_dtype):\n\u001b[0m\u001b[1;32m    968\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/envs/world_model_tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_compute_dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m   \u001b[0;31m# TODO(reedwm): Expose this property?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2012\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2013\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_compute_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \"\"\"The layer's compute dtype.\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_dir_name = \"sac_halfcheetah_1\"\n",
    "save_dir_path = os.path.join(\"results\", save_dir_name)\n",
    "\n",
    "!rm -r $save_dir_path\n",
    "trained_policy = train(save_dir_path)\n",
    "# # trained_policy = train(\"results/sac_halfcheetah_1\", checkpoint_dir=\"results/sac_halfcheetah_1/checkpoints\", train_iterations=168)\n",
    "\n",
    "eval_env = suite_pybullet.load(\"HalfCheetahBulletEnv-v0\")\n",
    "evaluate_policy_and_save_video(eval_env, trained_policy, os.path.join(save_dir_path, \"evaluation.mp4\"), num_episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
